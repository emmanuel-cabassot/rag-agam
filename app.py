import gradio as gr
import os
import numpy as np
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_ollama.llms import OllamaLLM
from langchain.chains import RetrievalQA

# Charger les variables d'environnement
load_dotenv()

# âœ… Charger les embeddings Hugging Face (BAAI/bge-large-en)
bge_embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en",
    encode_kwargs={'normalize_embeddings': True}
)

# âœ… Charger l'index FAISS avec les bons embeddings
index_path = "index_agam"
vectorstore = FAISS.load_local(index_path, bge_embeddings, allow_dangerous_deserialization=True)

# âœ… VÃ©rifier la dimension de l'index FAISS
faiss_dim = vectorstore.index.d
print(f"ğŸ“Š Dimension des vecteurs FAISS : {faiss_dim}")

# âœ… Configurer le modÃ¨le LLM Ollama avec DeepSeek-R1
llm = OllamaLLM(
    model="deepseek-r1",
    base_url="http://host.docker.internal:11434",  # URL pour accÃ©der Ã  Ollama depuis le conteneur Docker
    request_timeout=60  # Timeout augmentÃ© Ã  60 secondes
)


qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever(search_kwargs={"k": 15}))

# âœ… Fonction amÃ©liorÃ©e pour inclure les sources
def chatbot(question):
    # ğŸ” VÃ©rifier la dimension du vecteur de la requÃªte
    query_vector = np.array(bge_embeddings.embed_query(question), dtype=np.float32).reshape(1, -1)
    print(f"ğŸ“Š Dimension du vecteur de la requÃªte : {query_vector.shape}")

    # ğŸ” RÃ©cupÃ©rer les documents pertinents avec FAISS
    retrieved_docs = vectorstore.similarity_search_by_vector(query_vector[0], k=10)

    if not retrieved_docs:
        return "âŒ Aucun document pertinent trouvÃ©."


    # ğŸ“„ Construire un texte consolidÃ© Ã  partir des documents rÃ©cupÃ©rÃ©s
    retrieved_text = "\n".join(
        f"[{i+1}] {doc.metadata.get('source', 'Source inconnue')}: {doc.page_content}"
        for i, doc in enumerate(retrieved_docs[:10])
    )
    print(f"Retour du FAISS  avant: {retrieved_text}")

    retrieved_text = retrieved_text.replace("â€¢", "").replace("ïƒ˜", "").replace("\n\n", "\n").strip()
    print(f"Retour du FAISS aprÃ¨s : {retrieved_text}")


    # ğŸ¯ GÃ©nÃ©rer une rÃ©ponse avec Ollama en incluant les sources
    prompt = (
        f"Tu es un expert en urbanisme et tu as la connaissance de tous les documents.\n"
        f"RÃ©ponds prÃ©cisÃ©ment Ã  la question posÃ©e en t'appuyant uniquement sur les extraits ci-dessous.\n"
        f"N'oublie pas d'ajouter des rÃ©fÃ©rences aux documents d'oÃ¹ proviennent les informations.\n"
        f"\nExtraits des documents :\n{retrieved_text}\n\n"
        f"Question : {question}\n"
        f"RÃ©ponse dÃ©taillÃ©e avec sources :"
    )

    response = llm.invoke(prompt)

    # âœ… Extraire uniquement le texte gÃ©nÃ©rÃ© sans les mÃ©tadonnÃ©es
    print(response)

    return response["choices"][0]["message"]["content"]

# âœ… Interface Gradio amÃ©liorÃ©e
iface = gr.Interface(
    fn=chatbot,
    inputs="text",
    outputs="text",
    title="Chatbot RAG - AGAM",
    description="Pose une question sur les documents de l'AGAM et obtiens une rÃ©ponse dÃ©taillÃ©e avec les sources.",
    theme="default"
)

# âœ… Lancer l'interface avec les paramÃ¨tres pour Docker
iface.launch(server_name="0.0.0.0", server_port=7860)
