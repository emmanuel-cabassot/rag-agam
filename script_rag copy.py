import os
import re
import fitz  # PyMuPDF
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    TextLoader, CSVLoader, Docx2txtLoader, UnstructuredMarkdownLoader,
    UnstructuredPowerPointLoader, UnstructuredExcelLoader, UnstructuredRTFLoader,
    JSONLoader, UnstructuredXMLLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Charger les variables d'environnement
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("‚ùå Cl√© API OpenAI non d√©finie. V√©rifiez le fichier .env.")

# D√©finir le dossier contenant les fichiers
DATA_DIR = "data"

# D√©finition des loaders pour diff√©rents formats de fichiers
EXTENSION_LOADERS = {
    ".txt": TextLoader,
    ".csv": CSVLoader,
    ".docx": Docx2txtLoader,
    ".md": UnstructuredMarkdownLoader,
    ".pptx": UnstructuredPowerPointLoader,
    ".xls": UnstructuredExcelLoader,
    ".xlsx": UnstructuredExcelLoader,
    ".rtf": UnstructuredRTFLoader,
    ".json": JSONLoader,
    ".xml": UnstructuredXMLLoader
}


print(f"üîç Recherche de fichiers dans {DATA_DIR}...")

documents = []

# Fonction pour extraire le texte et les tableaux d'un PDF avec PyMuPDF
def extract_text_and_tables_pymupdf(pdf_path):
    """ Extrait le texte et les tableaux d'un PDF avec PyMuPDF et nettoie les donn√©es. """
    doc = fitz.open(pdf_path)
    full_text = []
    tables = []

    for page in doc:
        text = page.get_text("text")  # Extrait le texte brut
        text = clean_text(text)
        if text.strip():
            full_text.append(text)

    return "\n\n".join(full_text)

# Fonction de nettoyage avanc√© du texte
def clean_text(text):
    """ Nettoie et reformate le texte extrait d'un PDF """
    # Correction des mots coll√©s
    text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)

    # Ajout d'un espace apr√®s les tirets de liste
    text = re.sub(r"(?<!\n)-([A-Za-z])", r"- \1", text)

    # Suppression des espaces inutiles avant/apr√®s
    text = text.strip()

    # Ajout de sauts de ligne apr√®s les titres en majuscules
    text = re.sub(r"(\n[A-Z ]{5,})\n", r"\1\n\n", text)

    return text

# Fonction pour charger un fichier donn√©
def load_file(filepath):
    ext = os.path.splitext(filepath)[1].lower()
    
    if ext == ".pdf":
        text = extract_text_and_tables_pymupdf(filepath)
        return [Document(page_content=text)] if text.strip() else []

    if ext in EXTENSION_LOADERS:
        try:
            loader = EXTENSION_LOADERS[ext](filepath)
            return loader.load()
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement de {filepath} : {e}")
    
    return []

# Parcourir les fichiers dans le r√©pertoire DATA_DIR
file_count = 0
for root, _, files in os.walk(DATA_DIR):
    for file in files:
        file_count += 1
        filepath = os.path.join(root, file)
        print(f"üìÑ Chargement du fichier {file_count} : {filepath}")
        loaded_docs = load_file(filepath)
        documents.extend(loaded_docs)

print(f"üìÇ **Total de documents charg√©s : {len(documents)}**")

# Afficher un extrait des documents avant le split
print("\nüîç **Exemple de texte extrait :**")
print(documents[0].page_content[:1000])

# Ajout du Text Splitter avec une meilleure configuration
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1200,  # Segments plus longs pour plus de contexte
    chunk_overlap=200
)

split_documents = text_splitter.split_documents(documents)
print(f"üìÇ **Total de segments apr√®s d√©coupage : {len(split_documents)}**")

# Enregistrement des segments dans un fichier texte unique
output_file = "documents_transformes.txt"
with open(output_file, "w", encoding="utf-8") as f:
    for i, doc in enumerate(split_documents):
        f.write(f"--- Segment {i+1} ---\n")
        f.write(doc.page_content)
        f.write("\n\n")

print(f"‚úÖ Segments enregistr√©s dans le fichier '{output_file}'.")

# V√©rification de l'index FAISS
index_path = "index_agam"

# Suppression et recr√©ation compl√®te de l'index FAISS
if os.path.exists(index_path):
    print("üõ† Suppression de l'index FAISS existant...")
    os.system(f"rm -r {index_path}")

print("‚ö†Ô∏è Cr√©ation d'un nouvel index FAISS...")

# Initialiser les embeddings BGE
bge_embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en",
    encode_kwargs={'normalize_embeddings': True}
)

# Cr√©er le vectorstore avec les nouveaux embeddings
vectorstore = FAISS.from_documents(split_documents, bge_embeddings)
vectorstore.save_local(index_path)
print("‚úÖ Index FAISS enregistr√© dans 'index_agam/' !")

# V√©rifier le nombre de vecteurs dans l'index
print(f"üîç Nombre de vecteurs dans l'index FAISS : {vectorstore.index.ntotal}")

# Test de r√©cup√©ration et g√©n√©ration de r√©ponse
print("\nüîç Test du syst√®me RAG...")

# Nouvelle question pour tester le syst√®me
query = "Quels sont les d√©fis et opportunit√©s pour l'industrie du cin√©ma √† Marseille ?"

# Augmentation du nombre de documents r√©cup√©r√©s (k=15)
retriever = vectorstore.as_retriever(search_kwargs={"k": 15})

# Initialiser le mod√®le LLM
llm = ChatOpenAI(api_key=openai_api_key, model_name="gpt-4", temperature=0, request_timeout=15)

# Cr√©er la cha√Æne de r√©cup√©ration QA
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

# R√©cup√©rer les documents pertinents
retrieved_docs = retriever.get_relevant_documents(query)
cleaned_text = "\n".join(doc.page_content for doc in retrieved_docs).strip()

# G√©n√©rer la r√©ponse
response = llm.invoke([HumanMessage(content=f"En te basant uniquement sur ces extraits, r√©sume les d√©fis et opportunit√©s du cin√©ma √† Marseille :\n\n{cleaned_text}")])

print("\nüìù R√©ponse g√©n√©r√©e par l'IA :")
print(response.content)
